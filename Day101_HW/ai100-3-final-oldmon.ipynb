{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH  = '/kaggle/input/ml100-03-final/image_data/train'\n",
    "TEST_PATH = '/kaggle/input/ml100-03-final/image_data/test'\n",
    "OUTPUT_PATH = '/kaggle/working'\n",
    "NUM_CLASSES = 5\n",
    "NUM_EPOCHS = 32\n",
    "SEED = 77\n",
    "# saved model\n",
    "WEIGHTS_FINAL = 'model-InceptionResNetV2.h5'\n",
    "categories=os.listdir(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2260 images belonging to 5 classes.\n",
      "Found 563 images belonging to 5 classes.\n",
      "Class #0 = daisy\n",
      "Class #1 = dandelion\n",
      "Class #2 = rose\n",
      "Class #3 = sunflower\n",
      "Class #4 = tulip\n"
     ]
    }
   ],
   "source": [
    "# Image preprocess\n",
    "train_datagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.125, height_shift_range=0.125, zoom_range=0.125, horizontal_flip=True,\n",
    "                                   validation_split=0.2, rescale=1. / 255)\n",
    "train_batches = train_datagen.flow_from_directory(DATASET_PATH, subset = 'training', seed = SEED)\n",
    "valid_batches = train_datagen.flow_from_directory(DATASET_PATH, subset = 'validation', seed = SEED)\n",
    "for cls, idx in train_batches.class_indices.items():\n",
    "    print('Class #{} = {}'.format(idx, cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "net = InceptionResNetV2(include_top=False, input_shape=(256, 256, 3))\n",
    "x = net.output\n",
    "x = Flatten()(x)\n",
    "# Add Dense layer, each probability by softmax\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)\n",
    "net_final = Model(inputs=net.input, outputs=output_layer)\n",
    "net_final.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "71/71 [==============================] - 188s 3s/step - loss: 1.5067 - accuracy: 0.7058 - val_loss: 35942.2539 - val_accuracy: 0.2007\n",
      "Epoch 2/32\n",
      "71/71 [==============================] - 63s 887ms/step - loss: 1.6534 - accuracy: 0.7438 - val_loss: 445303904.0000 - val_accuracy: 0.2114\n",
      "Epoch 3/32\n",
      "71/71 [==============================] - 65s 910ms/step - loss: 1.1558 - accuracy: 0.7252 - val_loss: 492.6474 - val_accuracy: 0.2131\n",
      "Epoch 4/32\n",
      "71/71 [==============================] - 64s 906ms/step - loss: 1.3593 - accuracy: 0.7292 - val_loss: 1.7936 - val_accuracy: 0.6590\n",
      "Epoch 5/32\n",
      "71/71 [==============================] - 65s 909ms/step - loss: 0.7004 - accuracy: 0.7956 - val_loss: 2.6283 - val_accuracy: 0.7265\n",
      "Epoch 6/32\n",
      "71/71 [==============================] - 65s 909ms/step - loss: 0.6609 - accuracy: 0.8119 - val_loss: 0.6273 - val_accuracy: 0.7496\n",
      "Epoch 7/32\n",
      "71/71 [==============================] - 65s 916ms/step - loss: 0.4927 - accuracy: 0.8363 - val_loss: 0.5745 - val_accuracy: 0.7869\n",
      "Epoch 8/32\n",
      "71/71 [==============================] - 64s 899ms/step - loss: 0.4350 - accuracy: 0.8381 - val_loss: 8.9088 - val_accuracy: 0.5488\n",
      "Epoch 9/32\n",
      "71/71 [==============================] - 64s 906ms/step - loss: 0.4732 - accuracy: 0.8553 - val_loss: 8.7883 - val_accuracy: 0.4298\n",
      "Epoch 10/32\n",
      "71/71 [==============================] - 65s 910ms/step - loss: 0.4022 - accuracy: 0.8580 - val_loss: 0.5271 - val_accuracy: 0.8171\n",
      "Epoch 11/32\n",
      "71/71 [==============================] - 65s 917ms/step - loss: 0.3332 - accuracy: 0.8872 - val_loss: 0.4627 - val_accuracy: 0.8082\n",
      "Epoch 12/32\n",
      "71/71 [==============================] - 65s 909ms/step - loss: 0.3229 - accuracy: 0.8938 - val_loss: 0.1508 - val_accuracy: 0.8490\n",
      "Epoch 13/32\n",
      "71/71 [==============================] - 64s 907ms/step - loss: 0.3134 - accuracy: 0.8872 - val_loss: 0.1372 - val_accuracy: 0.8472\n",
      "Epoch 14/32\n",
      "71/71 [==============================] - 64s 906ms/step - loss: 0.2315 - accuracy: 0.9177 - val_loss: 0.9915 - val_accuracy: 0.8561\n",
      "Epoch 15/32\n",
      "71/71 [==============================] - 65s 913ms/step - loss: 0.2078 - accuracy: 0.9248 - val_loss: 0.2904 - val_accuracy: 0.8544\n",
      "Epoch 16/32\n",
      "71/71 [==============================] - 65s 920ms/step - loss: 0.2233 - accuracy: 0.9239 - val_loss: 0.3401 - val_accuracy: 0.8757\n",
      "Epoch 17/32\n",
      "71/71 [==============================] - 65s 917ms/step - loss: 0.2028 - accuracy: 0.9283 - val_loss: 0.0791 - val_accuracy: 0.8224\n",
      "Epoch 18/32\n",
      "71/71 [==============================] - 65s 910ms/step - loss: 0.1814 - accuracy: 0.9367 - val_loss: 1.1771 - val_accuracy: 0.7851\n",
      "Epoch 19/32\n",
      "71/71 [==============================] - 64s 907ms/step - loss: 0.1892 - accuracy: 0.9412 - val_loss: 0.2871 - val_accuracy: 0.8508\n",
      "Epoch 20/32\n",
      "71/71 [==============================] - 65s 921ms/step - loss: 0.2208 - accuracy: 0.9252 - val_loss: 1.0213 - val_accuracy: 0.7762\n",
      "Epoch 21/32\n",
      "71/71 [==============================] - 65s 918ms/step - loss: 0.2263 - accuracy: 0.9226 - val_loss: 0.6438 - val_accuracy: 0.8277\n",
      "Epoch 22/32\n",
      "71/71 [==============================] - 65s 915ms/step - loss: 0.1875 - accuracy: 0.9385 - val_loss: 0.1524 - val_accuracy: 0.8526\n",
      "Epoch 23/32\n",
      "71/71 [==============================] - 65s 912ms/step - loss: 0.1599 - accuracy: 0.9442 - val_loss: 0.2150 - val_accuracy: 0.8774\n",
      "Epoch 24/32\n",
      "71/71 [==============================] - 65s 915ms/step - loss: 0.1316 - accuracy: 0.9535 - val_loss: 0.7048 - val_accuracy: 0.8384\n",
      "Epoch 25/32\n",
      "71/71 [==============================] - 65s 918ms/step - loss: 0.1107 - accuracy: 0.9619 - val_loss: 0.5136 - val_accuracy: 0.8508\n",
      "Epoch 26/32\n",
      "71/71 [==============================] - 65s 910ms/step - loss: 0.1211 - accuracy: 0.9588 - val_loss: 0.6638 - val_accuracy: 0.8668\n",
      "Epoch 27/32\n",
      "71/71 [==============================] - 65s 912ms/step - loss: 0.0862 - accuracy: 0.9712 - val_loss: 0.3442 - val_accuracy: 0.8668\n",
      "Epoch 28/32\n",
      "71/71 [==============================] - 65s 914ms/step - loss: 0.1556 - accuracy: 0.9518 - val_loss: 321.7531 - val_accuracy: 0.5435\n",
      "Epoch 29/32\n",
      "71/71 [==============================] - 65s 909ms/step - loss: 0.2431 - accuracy: 0.9292 - val_loss: 0.1251 - val_accuracy: 0.8828\n",
      "Epoch 30/32\n",
      "71/71 [==============================] - 66s 924ms/step - loss: 0.1360 - accuracy: 0.9611 - val_loss: 0.3074 - val_accuracy: 0.8721\n",
      "Epoch 31/32\n",
      "71/71 [==============================] - 64s 902ms/step - loss: 0.1313 - accuracy: 0.9575 - val_loss: 0.2507 - val_accuracy: 0.8810\n",
      "Epoch 32/32\n",
      "71/71 [==============================] - 65s 918ms/step - loss: 0.0912 - accuracy: 0.9743 - val_loss: 0.2731 - val_accuracy: 0.8863\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "net_final.fit_generator(train_batches, validation_data = valid_batches, epochs = NUM_EPOCHS)\n",
    "# Store Model\n",
    "net_final.save(WEIGHTS_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "out = np.array(['id', 'flower_class'])\n",
    "testfiles=os.listdir(TEST_PATH)\n",
    "for testfile in testfiles:\n",
    "    filename = testfile.split('.')[0]\n",
    "    img = image.load_img(TEST_PATH+'/'+testfile,target_size=(256, 256))\n",
    "    if img is None:\n",
    "        continue\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = x /255.\n",
    "    pred = net_final.predict(x)[0]\n",
    "    tof=np.argmax(pred)\n",
    "    out = np.vstack((out,[filename, tof]))\n",
    "\n",
    "pd.DataFrame(out).to_csv(OUTPUT_PATH+'/prediction.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
